{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/jdownes/envs/summarization/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3' \n",
    "# RUN ON CPU\n",
    "# and do a time hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/jdownes/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"<Enter your token here>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = open(\"test.txt\", \"r\")\n",
    "content = i.read()\n",
    "\n",
    "i.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _summarize(text,max_tokens, model, tokenizer):\n",
    "    B_INST, E_INST = \"[INST] \", \" [/INST]\"\n",
    "    prompt = f\"{B_INST}Write a concise summary of the following text:\\n\\n[TEXT_START]\\n\\n{text}\\n\\n[TEXT_END]\\n\\n{E_INST}\"\n",
    "   \n",
    "    inputs = tokenizer( prompt,return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_tokens, use_cache=True, do_sample=True,temperature=0.2, top_p=0.95)\n",
    "\n",
    "    prompt_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    summary = tokenizer.decode(outputs[0][prompt_length:-1])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk tokens into list of texts\n",
    "def chunk(tokens, max_token_length,tokenizer):\n",
    "    token_length = len(tokens)\n",
    "    k = math.ceil(token_length /max_token_length)\n",
    "    chunk_sizes = [token_length // k + (1 if x < token_length % k else 0)  for x in range (k)]\n",
    "    #print(token_length, k, chunk_sizes)\n",
    "    last = 1\n",
    "    texts =[]\n",
    "    for l in chunk_sizes:\n",
    "        sub_sequence_ids = tokens[last: last+l]\n",
    "        last +=l\n",
    "        texts.append(tokenizer.decode(sub_sequence_ids))\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, cache_dir = None):\n",
    "    max_token_length = 1000\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", model_max_length = max_token_length, cache_dir= cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", pad_token_id = tokenizer.eos_token_id,cache_dir= cache_dir, device_map=\"auto\")\n",
    "    model.half()\n",
    "    \n",
    "    tokens = tokenizer.encode( text)#,return_tensors=\"pt\")\n",
    "    summary = \"\"\n",
    "    complete_runthroughs = 0\n",
    "    while len(tokens) > max_token_length:\n",
    "        texts = chunk(tokens, max_token_length, tokenizer)\n",
    "        \n",
    "        summaries = []\n",
    "        for text in texts:\n",
    "            sub_summary = _summarize(text, max_token_length, model, tokenizer)\n",
    "            summaries.append(sub_summary)\n",
    "        summary = \" \".join(summaries)\n",
    "            \n",
    "        tokens = tokenizer.encode(summary)\n",
    "        complete_runthroughs+=1\n",
    "        print(f\"run through entire text doc {complete_runthroughs} times\")\n",
    "    return _summarize(summary, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.95s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19626 > 1000). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/data2/jdownes/envs/summarization/lib/python3.8/site-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run through entire text doc 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run through entire text doc 2 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run through entire text doc 3 times\n"
     ]
    }
   ],
   "source": [
    "result = summarize(content, cache_dir = \"/data2/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary for test doc that is 10261 words, Approx 21 pages\n",
      "reduced to approximately 536 words\n",
      "The \"war on terror\" is a global military campaign initiated by the United States following the September 11 attacks, targeting militant Islamist movements and other groups. The conflict has caused significant displacement and death and has been criticized for its morality, tactics, and impact on civil liberties and human rights. The term \"war on terror\" persists in U.S. politics, despite efforts by the Obama administration to replace it with \"Overseas Contingency Operation.\" The text provides an overview of several military operations conducted by the United States and its allies against terrorist groups in different regions of the world, including Operation Enduring Freedom in Afghanistan, Pacific Eagle Philippines, and Operation Enduring Freedom Trans Sahara. The text also mentions other military operations, such as the Battle of Ras Kamboni in Somalia and the use of drones by the CIA in Pakistan to carry out operations associated with the Global War on Terror. The text discusses various counter-terrorism efforts by the United States, NATO, and other countries, including the deployment of 300 soldiers to Cameroon to fight against ISIL insurgency and a drone strike that killed Ilyas Kashmiri, a prominent al-Qaeda member. The text discusses the various responses to the 9/11 attacks, including legal challenges, resolutions, and military strategies. The United States has been accused of human rights violations in Guantanamo Bay and the use of \"floating prisons.\" The text also discusses the existence of secret prisons, known as \"black sites,\" across Europe and the United States, which were used for covert activities and temporary detention of high-value targets. The text estimates that between 518,000 and 549,000 people have died directly from the post-9/11 wars in Pakistan, Afghanistan, Iraq, Syria, Yemen, Somalia, and the Philippines, with many more dying from indirect effects such as water loss and disease. The conflict has caused the largest number of forced displacements by any single war since 1900, with the exception of World War II. The \"Costs of War\" project estimated that between 3.6 and 3.7 million indirect deaths have occurred in the post-9/11 war zones, with a total death toll of 4.5 to 4.6 million and rising. The report also estimated that over 38 million people have been displaced by the post-9/11 wars, with 26.7 million people returning home following displacement. The total number of US military combatants, contractors, and coalition troops killed in the wars is estimated to be around 7,052, 8,100, and 14,800, respectively, as of 2023. The total number of insurgent deaths since the commencement of the War on Terror in 2001 is generally estimated as being well into the hundreds of thousands, with hundreds of thousands of others captured or arrested. The War on Terror, which began in 2001 and is still ongoing, has resulted in the deaths of between 363,939 and 387,072 civilians in various war zones, according to a 2021 report. The total cost of the war, including operations and future veterans' care, is estimated to be $8 trillion between 2001 and 2022. The United States government has taken an interest in agricultural bioterrorism and has taken steps to prepare for threats from agricultural pathogens. The campaign has also been rebuked for being a perpetual war with no end-goal and for normalizing permanent violence as the status-quo.\n"
     ]
    }
   ],
   "source": [
    "print(f\"summary for test doc that is {content.count(' ')} words, Approx {round(content.count(' ') / 500)} pages\")\n",
    "print(f\"reduced to approximately {result.count(' ')} words\")\n",
    "print(result)\n",
    "with open(\"sample_out.txt\",\"w\") as f:\n",
    "    f.write(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
